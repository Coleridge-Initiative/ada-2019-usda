{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, Ursula Kaczmarek, Benjamin Feder.\n",
    "\n",
    "_source to be updated when notebook added to GitHub_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "JupyterLab contains a dynamic Table of Contents that can be accessed by clicking the last of the six icons on the left-hand sidebar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In an ideal world, we have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true, and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will explore some of the datasets we have on the ADRF to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. We will base our discussions around the following questions:\n",
    "\n",
    "__What are WIC households' total food expenditures in 2017? What is the share of WIC purchases for these households?__\n",
    "\n",
    "These questions provide the framework for _Sample Project 1_, which you can access in the shared folder on the ADRF.\n",
    "\n",
    "Within the scope of the questions, you will have an opportunity to explore different datasets in the ADRF, and this notebook will show you some ways you can analyze your data. This involves looking at basic metrics in the larger dataset, taking a subset of the data, creating derived variables, making sense of other variables, and so on. \n",
    "\n",
    "This will be done using both SQL and `pandas` (a Python package). The `pyathenajdbc` Python package provides a connection to Athena to pull data into Python. \n",
    "\n",
    "This notebook will provide an introduction and examples for:\n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to explore aggregate metrics\n",
    "- How to handle type conversions\n",
    "- How to join newly created tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "We will be using the `pyathenajdbc` Python package to access tables in our data store (Athena). \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- Subsetting data\n",
    "- `read_sql`\n",
    "- `head`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- Select data subsets\n",
    "- Sum over groups\n",
    "- Create new tables\n",
    "- Count distinct values of desired variables\n",
    "- Group data by chosen variables\n",
    "- Select a sub-sample of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Some Python packages include:\n",
    "- `numpy` is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object and a large suite of functions for doing numerical computing. \n",
    "- `pandas` is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- `pyathenajdbc` is a Python library for interfacing with Athena. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# Athena interaction imports\n",
    "from pyathenajdbc import connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a Python function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for example\n",
    "help(pd.read_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the Data\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and pandas in particular - make it much easier to calculate descriptive statistics of the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to access/read data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database or data store, or read directly from a URL (when you have internet access). Since we are working with the Athena data store `iri_usda` in this course, we will demonstrate how to use pandas to read data from a data store. For examples to read data from a CSV file, refer to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like running an SQL query in DBeaver, this function will ask for some information about the data store and the query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection, we will use the `Pyathenajdbc` package and tell it which data store we want to connect to, just like in DBeaver. Additional details on creating a connection to the data store are provided in the [Databases](01_1_Database_Connections.ipynb) notebook.\n",
    "\n",
    "> In your own work, you can reach out to your IT team to help get connected to your database. They should provide you with connection parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Database Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir = 's3://usda-iri-2019-queryresults/',\n",
    "               region_name = 'us-gov-west-1',\n",
    "               LogLevel = '0',\n",
    "               workgroup = 'workgroup-iri_usda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to pull demographic data on 20 households in 2017 enrolled in the WIC plan\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM iri_usda.demo_all\n",
    "WHERE wic_june = 1 and year = '2017'\n",
    "LIMIT 20;\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: When used together, the three quotation marks surrounding the query body is called a multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query` in Python, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `LIMIT` statement for two reasons. First, `LIMIT` helps users avoid running into memory issues in Python, as the command controls the maximum amount of rows of the dataframe. Second, it will also speed up some queries for the same reason. Generally, when performing an exploratory data analysis using SQL commands, we recommend you use `LIMIT` to look at a small sample of the data rather than wasting time and potentially creating memory issues by looking at the entire dataset. Sometimes, it may also be advantageous to provide robust `WHERE` clauses that will naturally limit the size of the output, such as restricting the resulting dataset to a specific year, or `state`, in this case. For instance, if you were curious how a given variable within the demographic data changed by year, you could start by restricting the dataset to just 2012 and then systematically change the year until you had a full sense of the trend (or lack thereof) in the dataset instead of grouping by the year from the start.\n",
    "\n",
    "> Note that `LIMIT` provides a simple way to get a \"sample\" of data. However, using `LIMIT` **does not provide a _random_ sample**; it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "\n",
    "Now that we have the two parameters (Athena connection and query), we can pass them to the `pd.read_sql()` function to obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function \n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first five rows of our 2017 WIC households pandas dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more information on df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Using Python and SQL\n",
    "\n",
    "Let's recall our guiding questions:\n",
    "\n",
    "__What are WIC households' total food expenditures in 2017? What is the share of WIC purchases for these households?__\n",
    "\n",
    "To find the answers to these questions, we will need to combine the demographics data with another available dataset. We will start slow and explore the two datasets individually, and then work up to answering these questions. Our process will work as follows:\n",
    "\n",
    "- Explore the available data tables\n",
    "- Check out the demographics table and some distributions within it\n",
    "- Define our mystery table\n",
    "- Explore mystery table\n",
    "- Combine datasets\n",
    "- Answer questions\n",
    "\n",
    ">Note: `demo_all` is a longitudinal data table, meaning that there may be multiple rows for one `panid`, or household, in the dataset if the household was tracked for multiple years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "\n",
    "As introduced in the [Databases](./02_1_Databases.ipynb) notebook, there are a few different ways to connect and explore the data in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tables, and Columns in database__\n",
    "\n",
    "Let's pull the list of table names in the database and the list of columns in these tables to get more familiar with the `iri_usda` datasbase. You only have read permissions for `iri_usda`, meaning you cannot create tables to this database. There is another database, `iri_usda_2019_db`, which you can directly write, or create tables, to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all available tables\n",
    "query = '''\n",
    "SHOW tables IN iri_usda;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names for a given table\n",
    "query = '''\n",
    "SHOW COLUMNS IN iri_usda.demo_all;\n",
    "'''\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h2>Checkpoint 1: Explore The Tables </h2></font>\n",
    "\n",
    "Use the following code cell to figure out which table could be one we will use later to answer our guiding questions. \n",
    "\n",
    "Are there any tables you can see being useful in your own work? **Discuss with your group.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check column names for a given table\n",
    "query = '''\n",
    "SHOW COLUMNS IN iri_usda.INSERT_TABLE\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dive into Demographics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at just Virginia households from the table `demo_all` within iri_usda, let's find the number of rows containing information from 2017. If you are confused about any of the variables and how they were encoded, please consult the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years for Virginia households in demo_all\n",
    "\n",
    "query = '''\n",
    "SELECT year\n",
    "from iri_usda.demo_all\n",
    "where state = 'VA'\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "#first five rows of df\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of Virginia households from 2017 in df\n",
    "\n",
    "print(len(df[df['year'] == '2017']))\n",
    "\n",
    "print(len(df.query('year == \"2017\"')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways to subset a `Pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function\n",
    "2. Create an array of `True` and `False` values with following format: `tables[\"column\"] == 'desired entry'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the `wic_june` column. Ignore the `wic_jan` covariate and focus solely on `wic_june` to identify if households are enrolled in the WIC program. If `wic_june` is coded as a 1, then the household is enrolled in the WIC program. First, we'll breakdown the households.\n",
    "\n",
    ">Note: The `projection61k` category contains survey weights for households the IRI believed had enough purchase entries so that they had a good grasp of the household's purchasing history. If `projection61k` is greater than zero, it means that there is enough purchasing data for that household to find general population estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct households in dataset in 2017\n",
    "query = '''\n",
    "SELECT COUNT(DISTINCT(panid)) as household_count\n",
    "FROM iri_usda.demo_all\n",
    "WHERE year = '2017';\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of distinct households enrolled in the wic program in 2017\n",
    "query = '''\n",
    "SELECT COUNT(DISTINCT(panid)) as wic_count\n",
    "FROM iri_usda.demo_all\n",
    "WHERE wic_june = 1 and year = '2017';\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of WIC households out of those with enough purchasing data in 2017\n",
    "query = ''' \n",
    "SELECT COUNT(DISTINCT(panid)) * 100.0 / \n",
    "    (\n",
    "    SELECT COUNT(DISTINCT(panid)) \n",
    "    FROM iri_usda.demo_all\n",
    "    where projection61k > 0 and year = '2017'\n",
    "    ) \n",
    "    as percentage_wic, COUNT(DISTINCT(panid)) as wic_total\n",
    "FROM iri_usda.demo_all\n",
    "where projection61k > 0 and wic_june = 1 and year = '2017'\n",
    "'''\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h2>Checkpoint 2: Take a Step Back </h2></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this sample compare to total WIC participation across the nation? **Discuss with your groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a more in-depth discussion about this sample and its representativeness of all households in the United States when we go through the Inference notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mystery Table Reveal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Necessary data__:\n",
    "- `iri_usda.demo_all`: individual household demographics data\n",
    "- `iri_usda.trip_all`: individual household purchase data by item and date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few specific questions to better understand `trip_all`:\n",
    "\n",
    "- How many distinct households are in `iri_usda.trip_all`?\n",
    "- How can you calculate the food expeditures for a single household at Walmart in 2017? What about the amount they've spent on WIC purchases? The `storename` value corresponding to Walmart is `3025`.\n",
    "- What are the most popular methods of payment at Walmart in 2017?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of households in the trips dataset\n",
    "query = \"\"\"\n",
    "SELECT count(distinct panid) as purchase_panids\n",
    "FROM iri_usda.trip_all;\n",
    "\"\"\"\n",
    "\n",
    "print(pd.read_sql(query, conn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __Large tables__ can take a long time to process on shared databases. The individual purchase table has more than 596.4 million records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Household Expenditures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the amount of money a single household spent at Walmart in 2017, we need to know:\n",
    "\n",
    "- How to calculate expenditures\n",
    "- Which columns we need to focus on\n",
    "- How to combine different SQL commands\n",
    "\n",
    "First, let's focus on calculating household expenditures for ten households. Within `trip_all`, there are two relevant variables for this question: `dollarspaid`, which is the cost of the individual item, and `coupon`, which is the amount the price was reduced through the usage of a coupon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 10 household expenditures\n",
    "query = \"\"\"\n",
    "SELECT panid, sum(dollarspaid) - sum(coupon) as total\n",
    "FROM iri_usda.trip_all\n",
    "GROUP BY panid\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entries of how much 10 households spent at Walmart in 2017\n",
    "query = \"\"\"\n",
    "SELECT panid, sum(dollarspaid) - sum(coupon) as total\n",
    "FROM iri_usda.trip_all\n",
    "WHERE year = '2017' and storename = 3025\n",
    "GROUP BY panid\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 10 household expenditures in 2017 at Walmart using WIC payment\n",
    "query = \"\"\"\n",
    "SELECT panid, sum(dollarspaid) - sum(coupon) as total\n",
    "FROM iri_usda.trip_all\n",
    "WHERE year = '2017' and storename = 3025 and mop = '7'\n",
    "GROUP by panid\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that some queries were faster than others? As we progress throughout the course, you will begin to run queries that may take much longer than five seconds to run. The more complicated the analysis is and the more data required to run the query affect the total runtime. But as discussed before, there are ways to easily cut down on runtime before creating exactly what you want using the entire data table. It is good practice to use `LIMIT` and/or `WHERE` clauses first to verify that the query is performing as intended before running a more extensive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of money spent by each possible method of payment at Walmart in 2017 rounded to two decimal places\n",
    "query = '''\n",
    "SELECT mop, round(count(*) * 100.0 / (SELECT count(*) \n",
    "    FROM iri_usda.trip_all WHERE year = '2017' and storename = 3025), 2) as percentage, \n",
    "    count(*) as count\n",
    "FROM iri_usda.trip_all\n",
    "WHERE year = '2017' and storename = 3025\n",
    "GROUP BY mop\n",
    "ORDER BY percentage DESC;\n",
    "'''\n",
    "# get results\n",
    "df_mop = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can see different columns in df_mop\n",
    "df_mop.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have explored all the information we need to combine to find the answer to the two questions. To do so, we will have to join the datasets based on their values for `panid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join `trip_all` with `demo_all`, we will need to match the two datasets by household, or `panid`. For this subset, we want to find the amount of money spent per WIC household with sufficient purchasing data. Before creating any sort of table, we would like to confirm that the join works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining data to get amount spent per household for those with sufficient purchasing data in 2017\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT demo.panid, round(sum(trip.dollarspaid) - sum(trip.coupon), 2) as total\n",
    "FROM iri_usda.demo_all demo\n",
    "LEFT JOIN iri_usda.trip_all trip\n",
    "ON trip.panid = demo.panid \n",
    "WHERE demo.wic_june = 1 and demo.projection61k > 0 and demo.year = '2017' and trip.year = '2017'\n",
    "GROUP BY demo.panid\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Athena, is impossible to create temporary tables, so we have to create regular tables and subsequently drop them if we are finished using them. In general, before creating a table, or temporary table, it is best practice to make sure the query runs as designed, and then assign it to a table. Here, a table is not necessary, but since we will be using the table for future calculations, we will create a table `panid_expense`, which contains each WIC-household `panid` with `projection61k > 0` for each year. From there, we can find the estimated amount of money spent in a given year by multiplying the amount spent by the survey weight.\n",
    "\n",
    "We will be writing the `panid_expense` table to `iri_usda_2019_db`, which is the Athena database we all have write access to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: We will discuss the difference in the expenditure calculations without weights further in the Inference notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see existing table list\n",
    "table_list = pd.read_sql('show tables IN iri_usda_2019_db;', conn)\n",
    "print(table_list)\n",
    "\n",
    "# get a series of tab_name values\n",
    "s = pd.Series(list(table_list['tab_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create money spent per household table for WIC households with sufficient purchasing data in 2017\n",
    "if('panid_expense' not in s.unique()):\n",
    "    query = \"\"\"\n",
    "    CREATE table iri_usda_2019_db.panid_expense\n",
    "    WITH (\n",
    "    format = 'Parquet',\n",
    "    parquet_compression = 'SNAPPY'\n",
    "    )\n",
    "    AS\n",
    "    SELECT demo.panid, round(sum(trip.dollarspaid) - sum(trip.coupon), 2) as total_wic_purchase, demo.projection61k\n",
    "    FROM iri_usda.demo_all demo \n",
    "    LEFT JOIN iri_usda.trip_all trip\n",
    "    ON trip.panid = demo.panid\n",
    "    WHERE demo.wic_june = 1 and demo.projection61k > 0 and trip.\"year\" = '2017' and demo.\"year\" = '2017'\n",
    "    GROUP BY demo.panid, demo.projection61k;\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out iri_usda_2019_db.panid_expense\n",
    "query = \"\"\"\n",
    "SELECT * \n",
    "FROM iri_usda_2019_db.panid_expense\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get descriptive stats for panid_expense\n",
    "query = '''\n",
    "select total_wic_purchase, projection61k\n",
    "from iri_usda_2019_db.panid_expense;\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have information for WIC household expenditures, we are quite close to the answer for the first question. How can we find an estimate for the total food expenditures for WIC households using the table we just created? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017 total food expenditures for WIC households\n",
    "query = '''\n",
    "SELECT sum(total_wic_purchase*projection61k) as part_1_answer\n",
    "FROM iri_usda_2019_db.panid_expense;\n",
    "'''\n",
    "\n",
    "full_purchase_sum = pd.read_sql(query, conn)\n",
    "print('WIC households spent approximately ${:.2f} in 2017.'.format(full_purchase_sum['part_1_answer'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (e.g. when building queries). Here are some other examples:\n",
    "\n",
    "1. Empty brackets to insert the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed.\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`).\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries like when creating \"labels\" and \"features\" for machine learning models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What share do WIC purchases make up for WIC households?\n",
    "\n",
    "By now, you should have a grasp of the demographic and purchasing datasets and also understand that we will need incorporate the `mop` variable as well.\n",
    "\n",
    "Our approach is as follows: We are going to find two separate values, one of estimated total purchase amount for WIC households (already created), and one of amount of estimated purchase amount using the WIC program for WIC households. From there, we can divide the two values to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder about part 1 calculated above: see answer to first question again\n",
    "query = '''\n",
    "SELECT sum(total_wic_purchase*projection61k) as part_1_answer\n",
    "FROM iri_usda_2019_db.panid_expense;\n",
    "'''\n",
    "\n",
    "full_purchase_sum = pd.read_sql(query, conn)\n",
    "print(full_purchase_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will select all of the households that were enrolled in the WIC program in 2017. After that, we will utilize a list comprehension to subset for trips just for these households in 2017 to extract their purchase data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of panids from demographic data that are in wic program from 2017\n",
    "query = '''\n",
    "SELECT DISTINCT panid\n",
    "FROM iri_usda.demo_all\n",
    "WHERE \"year\" = '2017' and wic_june = 1 and projection61k > 0;\n",
    "'''\n",
    "\n",
    "wic_hh = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to python string of strings\n",
    "wic_hh_sql = ','.join([\"'\"+id+\"'\" for id in wic_hh['panid'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This line of code may look complicated, so let's break it down step by step:\n",
    ">\n",
    "> 1. __`... for id in wic_hh['panid'].values ...`__ - Loop through every element `id` in the list `wic_hh['panid'].values`\n",
    "    > 2. __`... \"'\"+id+\"'\" ...`__ - String concatenation to make each `id` a string\n",
    "> 3. __`','.join( ... )`__ - Join each of these `id` strings into one string (a string of strings) with a comma separator\n",
    ">\n",
    "> _Additional Note: The formulation `[<action> for <item> in <iterable>]`is known as \"list comprehension\"._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same code can be duplicated in a classic `for` loop as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in wic_hh['panid'].values:\n",
    "    print(\"'\"+id+\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total dollars spent using WIC purchases for WIC customers in 2017 estimate\n",
    "query = '''\n",
    "select round(sum(visit.dollarspaid * demo.projection61k) - sum(visit.coupon * demo.projection61k), 2) as total\n",
    "from iri_usda.trip_all visit \n",
    "left join iri_usda.demo_all demo\n",
    "on visit.panid = demo.panid\n",
    "where visit.panid IN (\n",
    "    {}\n",
    "    )\n",
    "and visit.mop = '7' and demo.\"year\" = '2017' and visit.\"year\" = '2017'\n",
    "'''.format(wic_hh_sql)\n",
    "\n",
    "\n",
    "wic_purchase_sum = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wic_purchase_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h2>Checkpoint 3: Final Step </h2></font>\n",
    "    \n",
    "Calculate the estimated percentage of WIC purchases for WIC households. \n",
    "\n",
    "Note: There's more than one way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percentage of WIC purchases for WIC households\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Moving Forward__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of the guiding questions in these notebooks, we will extend these analyses a bit further in the sample project notebooks. The first sample project will build off of the analyses created in this notebook.\n",
    "\n",
    "Additionally, the data visualization notebook will contain a visualization of the average amount spent on a specific product across different family sizes per household member."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
