{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Disclosure Review Examples & Exercises_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with information on how to prepare research output for disclosure control. It outlines how to prepare different kind of outputs before submitting an export request and gives you an overview of the information needed for disclosure review. _Please read through the entire notebook because it will separately discuss all outputs that will be flagged in the disclosure review process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "%pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database interaction imports\n",
    "from pyathenajdbc import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir = 's3://usda-iri-2019-queryresults/',\n",
    "               region_name = 'us-gov-west-1',\n",
    "               LogLevel = '0',\n",
    "               workgroup = 'workgroup-iri_usda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Remarks on Disclosure Review\n",
    "\n",
    "## Files you can export\n",
    "In general, you can export any kind of file format. However, most researchers typically export tables, graphs, regression outputs and aggregated data. Thus, we ask you to export one of these types, which implies that every result you would like to export needs to be saved in either .csv, .txt or graph format.\n",
    "\n",
    "## Jupyter notebooks are only exported to retrieve code\n",
    "Unfortunately, you can't export results in a Jupyter notebook. Doing disclosure reviews on output in Jupyter notebooks is too burdensome for us. Jupyter notebooks will only be exported when the output is deleted for the purpose of exporting code. **This does not mean that you won't need your Jupyter notebooks during the export process.** \n",
    "\n",
    "## Documentation of code is important\n",
    "During the export process, we ask you to provide the code for every output you would like to export. It is important for the ADRF staff to have the code to better understand what you exactly did. Understanding how research results are created is important in understanding your research output. Thus, it is important to document every step of your analysis in your Jupyter notebook. \n",
    "\n",
    "## General rules to keep in mind\n",
    "A more detailed description of the rules for exporting results can be found on the class website. This is just a quick overview. You should go to the class website and read the entire guidelines (link below) before preparing your files for export. \n",
    "- The disclosure review is based on the underlying observations of your study. **Every statistic** you want to export should be based on at least three individual data points, and you must show the disclosure review team that every statistic you wish to export is based on at least three individual data points by providing counts in your input file.\n",
    "- Document your code so the reviewer can follow your data work. Assessing re-identification risks highly depends on the context. Therefore, it is important that you provide context info with your analysis for the reviewer.\n",
    "- Save the requested output with the corresponding code in your input and output folder. Make sure the code is executable. The code should exactly produce the output you requested.\n",
    "- If you are exporting powerpoint slides that show project results, you have to provide the code which produces the output in the slide.\n",
    "- Please export results only when they are final and you need them for your presentation or final project report.\n",
    "\n",
    "Documentation link: adrf.readthedocs.io/en/latest/export_of_results/guidelines.html#documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRI-Specific Requirements\n",
    "\n",
    "As mentioned in class, IRI has its own requirements to be able to release statistics generated from their datasets. We will cover these policies more extensively later in this notebook, but here is a summary of the aspects that will cause you to fail the disclosure review, listed for your convenience:\n",
    "- Micro data exports\n",
    "- Anything that identifies a particular product, brand, manufacturer, store, or retailer, especially sales volume or market share\n",
    "- De-identified data for a particular product, brand, store, or retailer that could be easily re-identified, such as\n",
    "    - Georgraphic/channel combinations, e.g. only one mass merchandiser in a particular MSA\n",
    "    - In a highly-concentrated industry, sales volume by a de-identified manufacturer could still be identifying for those in the industry\n",
    "    - Anything where the cell results are only drawn from one entity (product, brand, store, or retailer)\n",
    "- Specific UPC descriptions\n",
    "- Unweighted demographic makeup of the panel\n",
    "- Household-level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclosure Review Walkthrough\n",
    "\n",
    "We will use the provided IRI data to construct our statistics we are interested in and prepare them in a way so we can submit the output for disclosure review. Here, we will use code to find an estimate of 100% whole wheat bread purchases for WIC participants in 2016. This code will be a slight adaptation from the [machine learning preparation](04_01_ML_Data_Prep.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate an estimate of 100% whole wheat bread expenditures by all WIC households in 2016, we will create a data table where a row corresponds to a purchase in the `trip_all` table. From there, we can easily aggregate the sums to find the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your viewing pleasure, the following cells display the code we used to create two tables in the `iri_usda_2019_db` database, `disclosure_purchase` and `disclosure_final`. `disclosure_purchase` contains all 100% whole wheat bread purchases in 2016 with additional product details (reasoning will be provided for each variable's inclusion), and `disclosure_final` contains a subset of `disclosure_purchase` to just include the purchases for 2016 WIC participants with sufficient purchasing data and their corresponding sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table iri_usda_2019_db.disclosure_purchase\n",
    "        with(\n",
    "        format = 'Parquet',\n",
    "        parquet_compression = 'SNAPPY'\n",
    "        )\n",
    "        as\n",
    "        select t.panid, t.dollarspaid, t.coupon, t.upc, t.storename, p.manufacturer, p.brand\n",
    "        from iri_usda.pd_pos_all p, iri_usda.trip_all t\n",
    "        where p.upc = t.upc and t.year = '2016' and p.upcdesc like '%100% WHOLE WHEAT%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table iri_usda_2019_db.disclosure_final\n",
    "        with(\n",
    "        format = 'Parquet',\n",
    "        parquet_compression = 'SNAPPY'\n",
    "        )\n",
    "        as\n",
    "        select p.*, d.projection61k\n",
    "        from iri_usda.demo_all d \n",
    "        left join iri_usda_2019_db.disclosure_purchase p \n",
    "        on d.panid = p.panid\n",
    "        where d.wic_june = 1 and d.year = '2016' and d.projection61k > 0 and p.panid is not null "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data\n",
    "\n",
    "Let's see what `disclosure_final` looks like. Keep in mind that you cannot include any micro data outputs (i.e. `.head`). However, we will use the command, along with `df.info()` just to check our dataframe to give you a sense of the contents of `df`. We will also use `df.describe()`, which doesn't directly display micro data. However, **you cannot include the outputs of a regular .describe() command since the minimum, maximum and quartiles may represent individual rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "query = \"\"\"\n",
    "select *\n",
    "from iri_usda_2019_db.disclosure_final\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to check dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check basic stats of df\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration For Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find a dollar amount estimate of 100% whole wheat bread expenditures for WIC households in 2016. Recall that we can calculate the total cost of a product by subtracting `coupon` from `dollarspaid`. Thus, we need to start by creating a column that finds this difference for each product purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize total_cost column\n",
    "df['total_cost'] = df['dollarspaid'] - df['coupon']\n",
    "\n",
    "#confirm total_cost is calculated properly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to include the distribution of the `total_cost` category by in a numerical summary, you could not used the outputs from `.describe()`, as mentioned above. Instead, you would have to create _weighted_ fuzzy quartiles to represent the 25th, 50th and 75th quartiles (and any others you'd want to include). Let's walk through code to create these fuzzy quartiles. We will use the `.quantile()` function to find the true values for some quantiles. First, though, we need to find the amount of money each household spent on 100% whole wheat bread in the year and include their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find total costs by household\n",
    "temp = pd.DataFrame(df.groupby(['panid'])['total_cost'].sum())\n",
    "\n",
    "#find projection61k corresponding to each household\n",
    "# need to drop duplicates for df so it doesn't keep grabbing projection61k for each row in df\n",
    "weights_df = temp.merge(df[['panid', 'projection61k']].drop_duplicates(), 'right', on = 'panid') \n",
    "weights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate weighted dataframe by repeating the total_cost the amount of the weight for each household\n",
    "weighted_cost = np.repeat(weights_df['total_cost'], weights_df['projection61k'])\n",
    "#see first few\n",
    "weighted_cost[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign true quantiles around 25, 50 and 75 to true\n",
    "true = weighted_cost.quantile([.20, .30, .45, .55, .70, .80])\n",
    "# create list of all the fuzzy quantiles you want to calculate\n",
    "var = ['fuzzy_25', 'fuzzy_50', 'fuzzy_75']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find values for the fuzzy quantiles\n",
    "fq_25 = str((true[.20] + true[.30])/2)\n",
    "fq_50 = str((true[.45] + true[.55])/2)\n",
    "fq_75 = str((true[.70] + true[.80])/2)\n",
    "\n",
    "#save values in a second list of corresponding values\n",
    "val = [fq_25, fq_50, fq_75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in pandas dataframe\n",
    "fuzzy = pd.DataFrame(val, var)\n",
    "fuzzy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export these fuzzy quartiles as a csv, you can use the `to_csv` function and designate the file path and the name, which needs to end in .csv. Here, we will call the csv `fuzzy_statistic1`, since it is the first statistic we wish to export.\n",
    "\n",
    "> You just need to switch `benjaminfeder` in the file path to your username in order to save the csv in your home folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy.to_csv('/nfshome/benjaminfeder/fuzzy_statistic1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As proof that the underlying counts for the number of households, products, stores, brands and manufacturers were at least three for these fuzzy statistics, we can save the following cell as a csv `counts_statistic1` to designate that these counts correspond to our `fuzzy_statistic1` csv file. We can eventually include this csv file in our input folder for our export review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv of all counts we need\n",
    "data = {'n_unique_households': [df['panid'].nunique()], 'n_unique_prod': [df['upc'].nunique()], \n",
    "        'n_unique_store': [df['storename'].nunique()], 'n_unique_brand': [df['brand'].nunique()], \n",
    "        'n_unique_manufacturer': [df['manufacturer'].nunique()]}\n",
    "counts = pd.DataFrame(data)\n",
    "counts.to_csv('/nfshome/benjaminfeder/counts_statistic1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot a histogram of the distribution of amount of money spent on 100% whole wheat bread in our weighted sample. Luckily, we can use our `weights_df` dataframe to do this. The plot call will have three outputs, which will we assign accordingly: the counts of the bin sizes, the edges of the bins, and the actual graphical image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram\n",
    "counts, edges, graph = plt.hist(weights_df['total_cost'], weights = weights_df['projection61k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, given the default bin size of 10, we are not sure if each of these bins has at least three entries. To makes sure, we can check the `counts` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our weighted histogram, all the counts are at least three. Now let's check to see if the same holds for the unweighted histogram, because we need to make sure that each bin contains at least three unweighted households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot unweighted histogram\n",
    "counts, edges, graph = plt.hist(weights_df['total_cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that not all of our bin sizes are at least 3. Thus, we can manipulate our bins so that each bin contains at least three entries. To do so, we will combine the last four bins with help from our `edges` variable.\n",
    "\n",
    "> Note: There are other ways to adjust the bins. You can play around with the number of bins, drop outliers, or peform other manipulations. This is just one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at edges so we can combine the last four\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update edges to combine last four bins\n",
    "counts, edges, graph = plt.hist(weights_df['total_cost'], bins = [XX, XX, XX, XX, XX, XX, XX, XX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm counts are okay\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets save the unweighted counts and the resulting histogram so we can place these as evidence in our input folder before adding these bin changes to the weighted histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the barchart\n",
    "plt.hist(weights_df['total_cost'], bins = [XX, XX, XX, XX, XX, XX, XX, XX])\n",
    "plt.savefig('/nfshome/benjaminfeder/unweighted_hist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the counts of the unweighted hist\n",
    "pd.DataFrame(counts).to_csv('/nfshome/benjaminfeder/unweighted_hist_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for the histogram, we can update our weighted histogram with the edges from the unweighted histogram and save it so we can add it to our output folder later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted histogram with updated bin edges\n",
    "plt.hist(weights_df['total_cost'], weights = weights_df['projection61k'], bins = [XX, XX, XX, XX, XX, XX, XX, XX])\n",
    "plt.savefig('/nfshome/benjaminfeder/weighted_hist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another option, instead of creating a histogram, you can also release a density plot using the `distplot()` function from `seaborn`. The advantage of releasing a density plot here is that you do not need to reveal counts, but it also may not be as descriptive as a histogram. To create a weighted density plot, we can use the variable `weighted_cost` we created before.\n",
    "\n",
    "> You cannot use the `hist = True` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(weighted_cost, hist=False)\n",
    "plt.savefig('/nfshome/benjaminfeder/densityplot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating The Estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find our estimate for each household when weighted by multiplying `total_cost` by `projection61k` for each purchase in our original dataframe, `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weighted'] = df['total_cost'] * df['projection61k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take sum of weighted\n",
    "print('WIC households spent approximately ${:,.2f} in 2016 on 100% whole wheat products.'.format(sum(df['weighted'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to safely export this statistic since it is generated using the weights. **You cannot release any statistics, other than counts, for unweighted data.** However, we still need to generate a few more confirmations before this statistic is okay to release. Namely, we need to confirm that there are at least three and no dominance (constitutes a share of at least 80%) of the following:\n",
    "- Product\n",
    "- Store\n",
    "- Brand\n",
    "- Manufacturer\n",
    "\n",
    "Let's check to make sure our estimate follows the disclosure review guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount and dominance of products (upc)\n",
    "print(df['upc'].nunique())\n",
    "\n",
    "#check for dominance of upcs by selecting top five most represented upcs\n",
    "print(df['upc'].value_counts(normalize = True)[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount and dominance of stores (storename)\n",
    "print(df['storename'].nunique())\n",
    "\n",
    "#check for dominance of stores by selecting top five most represented stores\n",
    "print(df['storename'].value_counts(normalize = True)[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount and dominance of brand (brand)\n",
    "print(df['brand'].nunique())\n",
    "\n",
    "#check for dominance of brands by selecting top five most represented brands\n",
    "print(df['brand'].value_counts(normalize = True)[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check amount and dominance of manufacturer (manufacturer)\n",
    "print(df['manufacturer'].nunique())\n",
    "\n",
    "#check for dominance of manufacturers by selecting top five most represented manufacturers\n",
    "print(df['manufacturer'].value_counts(normalize = True)[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've shown that this statistic should pass disclosure review, where should you put your proof? As you will read in the documentation, there is an input file to include these statistics and other relevant counts. To load these in, we can save them all to two .csv files: (1) number of unique stores/brands/manufacturers/products and (2) proof of no dominance. The following code cells provide code to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv of all counts we need\n",
    "data = {'n_unique_prod': [df['upc'].nunique()], 'n_unique_store': [df['storename'].nunique()], \n",
    "        'n_unique_brand': [df['brand'].nunique()], 'n_unique_manufacturer': [df['manufacturer'].nunique()]}\n",
    "counts = pd.DataFrame(data)\n",
    "counts.to_csv('/nfshome/benjaminfeder/counts_estimate_stat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create csv of dominance proof by taking max of value_counts\n",
    "data = {'max_prod': [df['upc'].value_counts(normalize = True).max()],\n",
    "        'max_store': [df['storename'].value_counts(normalize = True).max()],\n",
    "        'max_brand': [df['brand'].value_counts(normalize = True).max()],\n",
    "        'max_manufacturer': [df['manufacturer'].value_counts(normalize = True).max()],\n",
    "        }\n",
    "dominance = pd.DataFrame(data)\n",
    "dominance.to_csv('/nfshome/benjaminfeder/dominance_estimate_stat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can easily add `dominance.csv` and `counts.csv` to your input folder for disclosure review. Let's also save our estimate in a separate csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'estimate': [sum(df['weighted'])]})\n",
    "data.to_csv('/nfshome/benjaminfeder/estimate.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Example from Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another example, let's say you wanted to explore `ml_model_train`, which contains every WIC and WIC-eligible household in 2014 and whether or not they purchased 100% whole wheat bread at least once in 2015, among other variables.\n",
    "\n",
    "> `ml_model_train` was created in the [Data Preparation](04_01_ML_Data_Prep.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select *\n",
    "from iri_usda_2019_db.ml_model_train\n",
    "'''\n",
    "\n",
    "df_train = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to visualize the proportion of households by their `wic_june` categorization for their `label` using a barchart. Recall that we can only show the proportion of households after they are weighted. To do so, we can add up the `projection61k` values after grouping by `wic_june` and `label`.\n",
    "\n",
    "> Since `projection61k` simply accounts for the amount of households in the general population one in our sample is representing, you can add `projection61k` to find weighted counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of weights by wic_june and label\n",
    "grouped = df_train.groupby(['wic_june', 'label'])['projection61k'].sum()\n",
    "print(grouped.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can generate the graph\n",
    "mygraph = grouped.unstack().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, because this barplot was generated based on weighted totals, we need to provide counts for both the weighted and unweighted populations. Clearly, we can see that the totals are all greater than three for each of the six groups, but just in case, we can add a `table=True` arguement to the plot call to display the underlying counts of the weighted table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation including underlying values: the option table=True displays the underlying counts\n",
    "mygraph = grouped.unstack().plot(kind='bar', table=True, figsize=(7,5), fontsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to show the counts of the unweighted table, recall that we can use a crosstab, as we did in the [machine learning](04_2_Machine_Learning.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute crosstab\n",
    "pd.crosstab(index=df_train['label'], columns=df_train['wic_june'])\n",
    "\n",
    "#save as csv\n",
    "pd.crosstab(index=df_train['label'], columns=df_train['wic_june']).to_csv('/nfshome/benjaminfeder/barchart_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have confirmed that the underlying unweighted counts are all at least three, we can export this graph as a pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can export the graph as pdf\n",
    "# Save plot to file\n",
    "export = mygraph.get_figure()\n",
    "export.set_size_inches(15,10, forward=True)\n",
    "export.savefig('/nfshome/benjaminfeder/sample_barchart.pdf', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder\n",
    "Every single item you wish to export, regardless of whether it is a .csv, .pdf, .png, or something else, must have corresponding proof in your input file to show that every group used to create this statistic followed our disclosure review rules.\n",
    "\n",
    "> We will add more examples in the coming weeks if they can be helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
